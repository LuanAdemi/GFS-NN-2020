{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LunarLander: Ein kleiner Einblick in Reinforcement Learning\n",
    "\n",
    "Das Ziel von Reinforcement Learning ist es, einen Agenten rein durch Beobachtung und Interaktion mit einer meist simulierten Umgebung,so zu trainieren, dass er ein Problem in dieser lösen kann. Hierbei versucht der Agent ständig seinen Reward zu erhöhen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "    # adds a transition tuple to the buffer    \n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    # returns a batch of the size batch_size, containing the past transitions\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dimension, \n",
    "        action_dimension, \n",
    "        max_action,\n",
    "        discount=0.99, \n",
    "        tau=0.005, \n",
    "        policy_noise=0.2, \n",
    "        noise_clip=0.5, \n",
    "        policy_frequency=2):\n",
    "        \n",
    "        \n",
    "        self.actor = Actor(state_dimension, action_dimension, max_action).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        \n",
    "        self.critic = Critic(state_dimension, action_dimension).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise \n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_frequency = policy_frequency\n",
    "        \n",
    "        \n",
    "        self.total_it = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1,-1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def train(self,replayBuffer, batch_size=200):\n",
    "        self.total_it += 1\n",
    "        \n",
    "        state, action, next_state, reward, not_done = replayBuffer.sample(batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # calculate the exploration noise a\n",
    "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            # select the action with the exploration noise a\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "            \n",
    "        \n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "        \n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        if self.total_it % self.policy_frequency == 0:\n",
    "            \n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, env_name, eval_episodes=10):\n",
    "\teval_env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\taction = policy.select_action(np.array(state))\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\n",
    "\t#print(\"---------------------------------------\")\n",
    "\t#print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "\t#print(\"---------------------------------------\")\n",
    "\treturn avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "state_dimension = env.observation_space.shape[0]\n",
    "action_dimension = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "B = ReplayBuffer(state_dimension, action_dimension)\n",
    "policy = TD3(state_dimension, action_dimension, float(env.action_space.high[0]))\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "\n",
    "render = False\n",
    "\n",
    "evaluations = [eval_policy(policy, env)]\n",
    "\n",
    "for t in range(int(1e6)):\n",
    "        \n",
    "        episode_timesteps += 1\n",
    "\n",
    "        # Select action randomly or according to policy\n",
    "        if t < 25e3:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = (\n",
    "                policy.select_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * 0.1, size=action_dimension)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        B.add(state, action, next_state, reward, done_bool)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= 25e3:\n",
    "            policy.train(B, 256)\n",
    "\n",
    "        if done: \n",
    "            # Reset environment\n",
    "            state, done = env.reset(), False\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "            \n",
    "            \n",
    "        # Evaluate episode\n",
    "        if (t + 1) % 5e3 == 0:\n",
    "            avg_reward = eval_policy(policy, env)\n",
    "            evaluations.append(avg_reward)\n",
    "            #if avg_reward >= 230:\n",
    "               # render = True\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(evaluations)\n",
    "            plt.title('Reward over episodes times ten')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
